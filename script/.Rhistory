scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
review.scores<- score.sentiment(df$text,pos.words,neg.words,.progress='text')
ggplot(review.scores, aes(x=score)) +
geom_histogram(binwidth=1) +
xlab("Sentiment score") +
ylab("Frequency") +
theme_bw()  +
theme(axis.title.x = element_text(vjust = -0.5, size = 14)) +
theme(axis.title.y=element_text(size = 14, angle=90, vjust = -0.25)) +
theme(plot.margin = unit(c(1,1,2,2), "lines"))
review.pos<- subset(review.scores,review.scores$score>= 2)
review.neg<- subset(review.scores,review.scores$score<= -2)
claim <- subset(review.scores, regexpr("narendramodi", review.scores$text) > 0)
ggplot(claim, aes(x = score)) + geom_histogram(binwidth = 1) + xlab("Sentiment score for the token 'narendramodi'") + ylab("Frequency") + theme_bw()  + theme(axis.title.x = element_text(vjust = -0.5, size = 14)) + theme(axis.title.y = element_text(size = 14, angle = 90, vjust = -0.25)) + theme(plot.margin = unit(c(1,1,2,2), "lines"))
class_emo = classify_emotion(df$Review, algorithm="bayes", prior=1.0)
class_emo = classify_emotion(df$text, algorithm="bayes", prior=1.0)
emotion = class_emo[,7]
emotion[is.na(emotion)] = "unknown"
class_pol = classify_polarity(df$Review, algorithm="bayes")
polarity = class_pol[,4]
class_pol = classify_polarity(df$text, algorithm="bayes")
polarity = class_pol[,4]
# data frame with results
sent_df = data.frame(text=df$text, emotion=emotion, polarity=polarity, stringsAsFactors=FALSE)
sent_df = within(sent_df, emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
ggplot(sent_df, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette="Dark2") +
labs(x="emotion categories", y="number of Feedback",
title = "Sentiment Analysis of Feedback about claim(classification by emotion)",
plot.title = element_text(size=12))
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="emotion categories", y="number of Feedback",
title = "Sentiment Analysis of Feedback about claim(classification by emotion)",
plot.title = element_text(size=12))
emos = levels(factor(sent_df$emotion))
nemo = length(emos)
emo.docs = rep("", nemo)
for (i in 1:nemo)
{
tmp = df$Review[emotion == emos[i]]
emo.docs[i] = paste(tmp, collapse=" ")
}
emo.docs = removeWords(emo.docs, stopwords("english"))
# create corpus
corpus = Corpus(VectorSource(emo.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = emos
# comparison word cloud
comparison.cloud(tdm, colors = brewer.pal(nemo, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
sent_df = data.frame(text=df$text, emotion=emotion, polarity=polarity, stringsAsFactors=FALSE)
sent_df = within(sent_df, emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
ggplot(sent_df, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette="Dark2") +
labs(x="emotion categories", y="number of Feedback",
title = "Sentiment Analysis of Feedback about claim(classification by emotion)",
plot.title = element_text(size=12))
df<- read.csv("../input/demonetization-tweets.csv", stringsAsFactors = FALSE)
length(unique(df$screenName))
df$created <- strptime(as.character(df$created), format = "%Y-%m-%d %H:%M")
df$day <- day(df$created)
df$hour <- hour(df$created)
corp <- Corpus(VectorSource(df$text))
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, removeWords, c("the", stopwords("english")))
corp <- tm_map(corp, PlainTextDocument)
corp.tdm <- TermDocumentMatrix(corp, control = list(minWordLength = 3))
corp.dtm <- DocumentTermMatrix(corp, control = list(minWordLength = 3))
wordcloud(corp, scale=c(5,0.5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, 'Dark2'))
hu.liu.pos = scan('../input/positive-words.txt', what = 'character',comment.char=';')
hu.liu.neg = scan('../input/negative-words.txt',what = 'character',comment.char= ';')
pos.words = c(hu.liu.pos)
neg.words = c(hu.liu.neg)
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list
# or a vector as an "l" for us
# we want a simple array ("a") of scores back, so we use
# "l" + "a" + "ply" = "laply":
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches= !is.na(pos.matches)
neg.matches= !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
review.scores<- score.sentiment(df$text,pos.words,neg.words,.progress='text')
ggplot(review.scores, aes(x=score)) +
geom_histogram(binwidth=1) +
xlab("Sentiment score") +
ylab("Frequency") +
theme_bw()  +
theme(axis.title.x = element_text(vjust = -0.5, size = 14)) +
theme(axis.title.y=element_text(size = 14, angle=90, vjust = -0.25)) +
theme(plot.margin = unit(c(1,1,2,2), "lines"))
review.pos<- subset(review.scores,review.scores$score>= 2)
review.neg<- subset(review.scores,review.scores$score<= -2)
modi <- subset(review.scores, regexpr("narendramodi", review.scores$text) > 0)
ggplot(modi, aes(x = score)) + geom_histogram(binwidth = 1) + xlab("Sentiment score for the token 'narendramodi'") + ylab("Frequency") + theme_bw()  + theme(axis.title.x = element_text(vjust = -0.5, size = 14)) + theme(axis.title.y = element_text(size = 14, angle = 90, vjust = -0.25)) + theme(plot.margin = unit(c(1,1,2,2), "lines"))
modi <- subset(review.scores, regexpr("modi", review.scores$text) > 0)
ggplot(modi, aes(x = score)) + geom_histogram(binwidth = 1) + xlab("Sentiment score for the token 'narendramodi'") + ylab("Frequency") + theme_bw()  + theme(axis.title.x = element_text(vjust = -0.5, size = 14)) + theme(axis.title.y = element_text(size = 14, angle = 90, vjust = -0.25)) + theme(plot.margin = unit(c(1,1,2,2), "lines"))
modi <- subset(review.scores, regexpr("blackmoney", review.scores$text) > 0)
ggplot(modi, aes(x = score)) + geom_histogram(binwidth = 1) + xlab("Sentiment score for the token 'narendramodi'") + ylab("Frequency") + theme_bw()  + theme(axis.title.x = element_text(vjust = -0.5, size = 14)) + theme(axis.title.y = element_text(size = 14, angle = 90, vjust = -0.25)) + theme(plot.margin = unit(c(1,1,2,2), "lines"))
modi <- subset(review.scores, regexpr("narendramodi", review.scores$text) > 0)
ggplot(modi, aes(x = score)) + geom_histogram(binwidth = 1) + xlab("Sentiment score for the token 'narendramodi'") + ylab("Frequency") + theme_bw()  + theme(axis.title.x = element_text(vjust = -0.5, size = 14)) + theme(axis.title.y = element_text(size = 14, angle = 90, vjust = -0.25)) + theme(plot.margin = unit(c(1,1,2,2), "lines"))
class_emo = classify_emotion(df$text, algorithm="bayes", prior=1.0)
emotion = class_emo[,7]
emotion[is.na(emotion)] = "unknown"
class_pol = classify_polarity(df$text, algorithm="bayes")
polarity = class_pol[,4]
# data frame with results
sent_df = data.frame(text=df$text, emotion=emotion, polarity=polarity, stringsAsFactors=FALSE)
# sort data frame
sent_df = within(sent_df, emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
ggplot(sent_df, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette="Dark2") +
labs(x="emotion categories", y="number of Feedback",
title = "classification by emotion",
plot.title = element_text(size=12))
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="emotion categories", y="number of Feedback",
title = "classification by emotion",
plot.title = element_text(size=12))
emos = levels(factor(sent_df$emotion))
nemo = length(emos)
emo.docs = rep("", nemo)
for (i in 1:nemo)
{
tmp = df$Review[emotion == emos[i]]
emo.docs[i] = paste(tmp, collapse=" ")
}
# remove stopwords
emo.docs = removeWords(emo.docs, stopwords("english"))
# create corpus
corpus = Corpus(VectorSource(emo.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = emos
comparison.cloud(tdm, colors = brewer.pal(nemo, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
emos = levels(factor(sent_df$emotion))
nemo = length(emos)
emo.docs = rep("", nemo)
for (i in 1:nemo)
{
tmp = df$text[emotion == emos[i]]
emo.docs[i] = paste(tmp, collapse=" ")
}
# remove stopwords
emo.docs = removeWords(emo.docs, stopwords("english"))
# create corpus
corpus = Corpus(VectorSource(emo.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = emos
# comparison word cloud
comparison.cloud(tdm, colors = brewer.pal(nemo, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
# Create a new column of random numbers in place of the usernames and redraw the plots
# find out how many random numbers we need
n <- length(unique(df$screenName))
# generate a vector of random number to replace the names, we'll get four digits just for convenience
randuser<- round(runif(n, 1000, 9999),0)
# match up a random number to a username
screenName<- unique(df$screenName)
screenName<- sapply(screenName, as.character)
randuser<- cbind(randuser, screenName)
# Now merge the random numbers with the rest of the Twitter data, and match up the correct random numbers with multiple instances of the usernames...
rand.df<-  merge(randuser, df, by="screenName")
# determine the frequency of tweets per account
counts<- table(rand.df$randuser)
# create an ordered data frame for further manipulation and plotting
countsSort<- data.frame(user = unlist(dimnames(counts)), count = sort(counts, decreasing = TRUE), row.names = NULL)
# create a subset of those who tweeted at least 5 times or more
countsSortSubset<- subset(countsSort,countsSort$count> 0)
# extract counts of how many tweets from each account were retweeted
# first clean the twitter messages by removing odd characters
rand.df$text<- sapply(rand.df$text,function(row) iconv(row,to = 'UTF-8'))
# remove @ symbol from user names
trim<- function (x) sub('@','',x)
# pull out who the message is to
require(stringr)
rand.df$to<- sapply(rand.df$text, function(name) trim(name))
# extract who has been retweeted
rand.df$rt<- sapply(rand.df$text, function(tweet)
trim(str_match(tweet,"^RT (@[[:alnum:]_]*)")[2]))
# replace names with corresponding anonymising number
randuser<- data.frame(randuser)
rand.df$rt.rand<- as.character(randuser$randuser)[match(as.character(rand.df$rt),
as.character(randuser$screenName))]
# make a table with anonymised IDs and number of RTs for each account
countRT<- table(rand.df$rt.rand)
countRTSort<- sort(countRT)
# subset those people RT?d at least twice
countRTSortSubset<- subset(countRTSort,countRT>2)
# create a data frame for plotting
countRTSortSubset.df<-data.frame(user = as.factor(unlist(dimnames(countRTSortSubset))), RT_count =
as.numeric(unlist(countRTSortSubset)))
# combine tweet and retweet counts into one data frame
countUser<- merge(randuser, countsSortSubset, by.x = "randuser", by.y = "user")
TweetRetweet<- merge(countUser, countRTSortSubset.df, by.x = "randuser", by.y = "user", all.x = TRUE)
# create a Cleveland dot plot of tweet counts and retweet counts per Twitter account
# solid data point = number of tweets, letter R = number of retweets
library(ggplot2)
library(grid)
ggplot() +
geom_point(data = TweetRetweet, mapping =  aes(reorder(randuser, count), count), size = 3) +
geom_point(data = TweetRetweet, mapping =  aes(randuser, RT_count), size = 4, shape = "R") +
xlab("Author") +
ylab("Number of messages") +
coord_flip() +
theme_bw() +
theme(axis.title.x = element_text(vjust = -0.5, size = 14)) +
theme(axis.title.y = element_text(size = 14, angle=90)) +
theme(plot.margin = unit(c(1,1,2,2), "lines"))
library(wordcloud)
library(tm)
library(plyr)
library(ggplot2)
library(grid)
library(sentiment)
library(Rgraphviz)
library(lubridate)
df<- read.csv("../input/demonetization-tweets.csv", stringsAsFactors = FALSE)
length(unique(df$screenName))
df$created <- strptime(as.character(df$created), format = "%Y-%m-%d %H:%M")
df$day <- day(df$created)
df$hour <- hour(df$created)
n <- length(unique(df$screenName))
randuser<- round(runif(n, 1000, 9999),0)
screenName<- unique(df$screenName)
screenName<- sapply(screenName, as.character)
randuser<- cbind(randuser, screenName)
rand.df<-  merge(randuser, df, by="screenName")
counts<- table(rand.df$randuser)
countsSort<- data.frame(user = unlist(dimnames(counts)), count = sort(counts, decreasing = TRUE), row.names = NULL)
countsSortSubset<- subset(countsSort,countsSort$count> 0)
rand.df$text<- sapply(rand.df$text,function(row) iconv(row,to = 'UTF-8'))
trim<- function (x) sub('@','',x)
# pull out who the message is to
require(stringr)
rand.df$to<- sapply(rand.df$text, function(name) trim(name))
# extract who has been retweeted
rand.df$rt<- sapply(rand.df$text, function(tweet)
trim(str_match(tweet,"^RT (@[[:alnum:]_]*)")[2]))
# replace names with corresponding anonymising number
randuser<- data.frame(randuser)
rand.df$rt.rand<- as.character(randuser$randuser)[match(as.character(rand.df$rt),
as.character(randuser$screenName))]
countRT<- table(rand.df$rt.rand)
countRTSort<- sort(countRT)
countRTSortSubset<- subset(countRTSort,countRT>2)
# create a data frame for plotting
countRTSortSubset.df<-data.frame(user = as.factor(unlist(dimnames(countRTSortSubset))), RT_count =
as.numeric(unlist(countRTSortSubset)))
countUser<- merge(randuser, countsSortSubset, by.x = "randuser", by.y = "user")
TweetRetweet<- merge(countUser, countRTSortSubset.df, by.x = "randuser", by.y = "user", all.x = TRUE)
# create a Cleveland dot plot of tweet counts and retweet counts per Twitter account
# solid data point = number of tweets, letter R = number of retweets
library(ggplot2)
library(grid)
ggplot() +
geom_point(data = TweetRetweet, mapping =  aes(reorder(randuser, count), count), size = 3) +
geom_point(data = TweetRetweet, mapping =  aes(randuser, RT_count), size = 4, shape = "R") +
xlab("Author") +
ylab("Number of messages") +
coord_flip() +
theme_bw() +
theme(axis.title.x = element_text(vjust = -0.5, size = 14)) +
theme(axis.title.y = element_text(size = 14, angle=90)) +
theme(plot.margin = unit(c(1,1,2,2), "lines"))
t <- as.data.frame(table(rand.df$randuser))
# make table with counts of retweets per person
rt<- as.data.frame(table(rand.df$rt.rand))
# combine tweet count and retweet count per person
t.rt<- merge(t,rt,by="Var1")
# creates new col and adds ratio tweet/retweet
t.rt["ratio"] <- t.rt$Freq.y / t.rt$Freq.x
# sort it to put names in order by ratio
sort.t.rt<- t.rt[order(t.rt$ratio),]
# exclude those with 2 tweets or less
sort.t.rt.subset<- subset(sort.t.rt,sort.t.rt$Freq.x>2)
#
# drop unused levels leftover from subsetting
sort.t.rt.subset.drop<- droplevels(sort.t.rt.subset)
# plot nicely ordered counts of tweets by person for
# people> 5 tweets
ggplot(sort.t.rt.subset.drop, aes(reorder(Var1, ratio), ratio)) +
xlab("Author") +
ylab("Ratio of messages retweeted by others to original messages")+
geom_point() +
coord_flip() +
theme_bw()  +
theme(axis.title.x = element_text(vjust = -0.5, size = 14)) +
theme(axis.title.y = element_text(size = 14, angle=90)) +
theme(plot.margin = unit(c(1,1,2,2), "lines"))
rt<- data.frame(user=rand.df$randuser, rt=rand.df$rt.rand)
# omit pairs with NA and get only unique pairs
rt.u<- na.omit(unique(rt)) #
# begin social network analysis plotting
require(igraph)
require (sna)
degree<- sna::degree
g <- graph.data.frame(rt.u, directed = F)
# plot a basic network graph, ready for further customisation:
plot.igraph(g)
# and here is a rough approximation of the published plot
plot(g,  			#the graph to be plotted
layout=layout.fruchterman.reingold,	# the layout method. see the igraph documentation for details
vertex.label.dist=0.5,			#puts the name labels slightly off the dots
vertex.frame.color='blue', 		#the color of the border of the dots
vertex.label.color='black',		#the color of the name labels
vertex.label.font=3,			#the font of the name labels
vertex.label.cex=0.5,			#specifies the size of the font of the labels. can also be made to vary
vertex.size = 0.5
)
g.wc<- walktrap.community(g, steps = 1000, modularity=TRUE)
plot(as.dendrogram(g.wc))
max(g.wc$membership)+1
require(tm)
a <- Corpus(VectorSource(df$text)) # create corpus object
a <- tm_map(a, tolower) # convert all text to lower case
a <- tm_map(a, removePunctuation)
a <- tm_map(a, removeNumbers)
a <- tm_map(a, stemDocument, language = "english") # converts terms to tokens
a <- tm_map(a, PlainTextDocument)
a.tdm<- TermDocumentMatrix(a, control = list(minWordLength = 3)) # create a term document matrix, keepiing only tokens longer than three characters, since shorter tokens are very hard to interpret
inspect(a.tdm[1:10,1:10]) # have a quick look at the term document matrix
findFreqTerms(a.tdm, lowfreq=30)
findAssocs(a.tdm, 'narendramodi', 0.3)
wordcloud(corp, scale=c(5,0.5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, 'Dark2'))
findFreqTerms(corp.tdm, lowfreq=30)
findAssocs(corp.tdm, 'narendramodi', 0.2)
corp.tdm.df <- data.frame(inspect(corp.tdm))
corp.tdm.df <- sort(rowSums(corp.tdm.df),decreasing=TRUE) # populate term frequency and sort in decesending order
df.freq <- data.frame(word = names(corp.tdm.df),freq=corp.tdm.df) # Table with terms and frequency
dark2 <- brewer.pal(6,"Dark2")
wordcloud(df.freq$word,df.freq$freq,min.freq=freqControl,random.order=FALSE, rot.per=0.2,colors=dark2,main="Corpus wordcloud")
hu.liu.pos = scan(text=getURL("https://github.com/ManoharSwamynathan/Sentiment_Analysis/blob/FirstCommit/input/positive-words.txt", what = 'character',comment.char=';') #load +ve sentiment word list
hu.liu.pos = scan(text=getURL("https://github.com/ManoharSwamynathan/Sentiment_Analysis/blob/FirstCommit/input/positive-words.txt"), what = 'character',comment.char=';')
hu.liu.pos = scan(text=getURL("https://github.com/ManoharSwamynathan/Sentiment_Analysis/blob/FirstCommit/input/positive-words.txt"), what = 'character',comment.char=';')
require(RCurl)
install.packages('RCurl')
hu.liu.pos = scan(text=getURL("https://github.com/ManoharSwamynathan/Sentiment_Analysis/blob/FirstCommit/input/positive-words.txt"), what = 'character',comment.char=';')
require(RCurl)
hu.liu.pos = scan(text=getURL("https://github.com/ManoharSwamynathan/Sentiment_Analysis/blob/FirstCommit/input/positive-words.txt"), what = 'character',comment.char=';')
hu.liu.pos = scan('D:/opinion-lexicon-English/positive-words.txt', what = 'character',comment.char=';') #load +ve sentiment word list
hu.liu.pos = scan('D:/opinion-lexicon-English/positive-words.txt', what = 'character',comment.char=';') #load +ve sentiment word list
hu.liu.pos = scan('../input/positive-words.txt', what = 'character',comment.char=';') #load +ve sentiment word list
hu.liu.pos = scan(text=getURL("https://github.com/ManoharSwamynathan/Sentiment_Analysis/blob/FirstCommit/input/positive-words.txt"), what = 'character',comment.char=';')
hu.liu.pos = scan(text=getURL("https://github.com/ManoharSwamynathan/Sentiment_Analysis/blob/FirstCommit/input/positive-words.txt"), what = 'character', comment.char=';')
df<- read.csv("../input/demonetization-tweets.csv", stringsAsFactors = FALSE)
df$text = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', df$text)
df$text = gsub('@\\w+', '', df$text)
# remove punctuation
df$text = gsub('[[:punct:]]', '', df$text)
# remove numbers
df$text = gsub('[[:digit:]]', '', df$text)
# remove html links
df$text = gsub('http\\w+', '', df$text)
# remove unnecessary spaces
df$text = gsub('[ \t]{2,}', '', df$text)
df$text = gsub('^\\s+|\\s+$', '', df$text)
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, 'error'))
y = tolower(x)
# result
return(y)
}
df$text = sapply(df$text, try.error)
df$text = df$text[!is.na(df$text)]
names(df$text) = NULL
class_emo = classify_emotion(df$text, algorithm='bayes', prior=1.0)
library(wordcloud)
library(tm)
library(plyr)
library(ggplot2)
library(grid)
library(sentiment)
library(Rgraphviz)
library(lubridate)
df<- read.csv("../input/demonetization-tweets.csv", stringsAsFactors = FALSE)
length(unique(df$screenName))
n <- length(unique(df$screenName))
# generate a vector of random number to replace the names, we'll get four digits just for convenience
randuser <- round(runif(n, 1000, 9999),0)
# match up a random number to a username
screenName <- unique(df$screenName)
screenName <- sapply(screenName, as.character)
randuser <- cbind(randuser, screenName)
# Now merge the random numbers with the rest of the Twitter data, and match up the correct random numbers with multiple instances of the usernames...
rand.df  <-  merge(randuser, df, by="screenName")
# determine the frequency of tweets per account
counts <- table(rand.df$randuser)
# create an ordered data frame for further manipulation and plotting
countsSort <- data.frame(user = unlist(dimnames(counts)), count = sort(counts, decreasing = TRUE), row.names = NULL)
# create a subset of those who tweeted at least 5 times or more
countsSortSubset <- subset(countsSort,countsSort$count > 0)
# extract counts of how many tweets from each account were retweeted
# first clean the twitter messages by removing odd characters
rand.df$text <- sapply(rand.df$text,function(row) iconv(row,to = 'UTF-8'))
# remove @ symbol from user names
trim <- function (x) sub('@','',x)
# pull out who the message is to
require(stringr)
rand.df$to <- sapply(rand.df$text, function(name) trim(name))
# extract who has been retweeted
rand.df$rt <- sapply(rand.df$text, function(tweet)
trim(str_match(tweet,"^RT (@[[:alnum:]_]*)")[2]))
# replace names with corresponding anonymising number
randuser <- data.frame(randuser)
rand.df$rt.rand <- as.character(randuser$randuser)[match(as.character(rand.df$rt),
as.character(randuser$screenName))]
# make a table with anonymised IDs and number of RTs for each account
countRT <- table(rand.df$rt.rand)
countRTSort <- sort(countRT)
# subset those people RTÂ’d at least twice
countRTSortSubset <- subset(countRTSort,countRT>2)
# create a data frame for plotting
countRTSortSubset.df <-data.frame(user = as.factor(unlist(dimnames(countRTSortSubset))), RT_count = as.numeric(unlist(countRTSortSubset)))
# combine tweet and retweet counts into one data frame
countUser <- merge(randuser, countsSortSubset, by.x = "randuser", by.y = "user")
TweetRetweet <- merge(countUser, countRTSortSubset.df, by.x = "randuser", by.y = "user", all.x = TRUE)
# create a Cleveland dot plot of tweet counts and retweet counts per Twitter account
# solid data point = number of tweets, letter R = number of retweets
require(ggplot2)
require(grid)
ggplot() +
geom_point(data = TweetRetweet, mapping =  aes(reorder(randuser, count), count), size = 3) +
geom_point(data = TweetRetweet, mapping =  aes(randuser, RT_count), size = 4, shape = "R") +
xlab("Author") +
ylab("Number of messages") +
coord_flip() +
theme_bw() +
theme(axis.title.x = element_text(vjust = -0.5, size = 14)) +
theme(axis.title.y = element_text(size = 14, angle=90)) +
theme(plot.margin = unit(c(1,1,2,2), "lines"))
users <- unique(df$screenName)
users <- sapply(users, as.character)
# make a data frame for further manipulation
users.df <- data.frame(users = users, followers = "", stringsAsFactors = FALSE)
# loop to populate users$followers with a follower count obtained from Twitter API
for (i in 1:nrow(users.df))
{
# tell the loop to skip a user if their account is protected
# or some other error occurs
result <- try(getUser(users.df$users[i])$followersCount, silent = FALSE);
if(class(result) == "try-error") next;
# get the number of followers for each user
users.df$followers[i] <- getUser(users.df$users[i])$followersCount
# tell the loop to pause for 60 s between iterations to
# avoid exceeding the Twitter API request limit
# this is going to take a long time if there are a lot
# of users, good idea to let it run overnight
print('Sleeping for 60 seconds...')
Sys.sleep(60);
}
# merge follower count with number of tweets per author
followerCounts <- merge(TweetRetweet, users.df, by.x = "screenName", by.y = "users")
# convert to value to numeric for further analysis
followerCounts$followers <- as.numeric(followerCounts$followers)
followerCounts$counts <-    as.numeric(followerCounts$counts)
# create a plot
ggplot(data = followerCounts, aes(count, followers)) +
geom_text(aes(label = randuser, size = RT_count)) +
scale_size(range=c(3,10)) +
scale_x_log10(breaks = c(10,20,40,60,80,100)) +
scale_y_log10(breaks = c(10,100,seq(1000,7000,1000))) +
xlab("Number of Messages") +
ylab("Number of Followers") +
theme_bw()  +
theme(axis.title.x = element_text(vjust = -0.5, size = 14)) +
theme(axis.title.y = element_text(size = 14, angle=90)) +
theme(plot.margin = unit(c(1,1,2,2), "lines"))
